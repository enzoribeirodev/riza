# RIZA - RAG Chat Interface

Uma soluÃ§Ã£o genÃ©rica e modular para sistemas RAG (Retrieval Augmented Generation) com foco em reprodutibilidade, personalizaÃ§Ã£o e eficiÃªncia. RIZA visa ser uma plataforma extensÃ­vel para conversaÃ§Ã£o com documentos, integrando mÃºltiplas tecnologias e provedores de LLM.

---

## ğŸ“‹ Ãndice

- [VisÃ£o Geral](#ğŸ¯-visÃ£o-geral)
- [Arquitetura](#ğŸ—ï¸-arquitetura)
- [Estrutura do Projeto](#ğŸ“-estrutura-do-projeto)
- [Funcionalidades](#âœ¨-funcionalidades)
- [InstalaÃ§Ã£o](#ğŸš€-instalaÃ§Ã£o)
- [Como Usar](#ğŸ’»-como-usar)
- [Stack TecnolÃ³gica](#ğŸ› ï¸-stack-tecnolÃ³gica)
- [Pipeline RAG](#ğŸ”„-pipeline-rag)
- [DecisÃµes TÃ©cnicas](#ğŸ¯-decisÃµes-tÃ©cnicas)
- [ConfiguraÃ§Ã£o AvanÃ§ada](#âš™ï¸-configuraÃ§Ã£o-avanÃ§ada)
- [PrÃ³ximos Passos](#ğŸš§-prÃ³ximos-passos)

---

## ğŸ¯ VisÃ£o Geral

RIZA Ã© uma prova de conceito (PoC) de um assistente de perguntas e respostas que utiliza documentos como fonte de verdade. O projeto demonstra a implementaÃ§Ã£o completa de um pipeline RAG, desde a ingestÃ£o de documentos atÃ© a geraÃ§Ã£o de respostas contextualizadas, com suporte a mÃºltiplos provedores de LLM e modelos de embedding.

### MotivaÃ§Ã£o

Inicialmente desenvolvido para Q&A sobre manuais tÃ©cnicos (testada inicialmente com o manual do Samsung Galaxy Z Flip 7), o projeto evoluiu para uma soluÃ§Ã£o genÃ©rica aplicÃ¡vel a diversos domÃ­nios, incluindo:

- AssistÃªncia tÃ©cnica e suporte
- AnÃ¡lise de documentos jurÃ­dicos
- Leitura e sÃ­ntese de artigos cientÃ­ficos
- DocumentaÃ§Ã£o corporativa
- Bases de conhecimento internas

### Diferencial

RIZA nÃ£o Ã© apenas mais um RAG bÃ¡sico. O projeto implementa:

- **Compression Retriever com Reranking**: Melhora significativa na qualidade dos documentos recuperados
- **Chunking hÃ­brido inteligente**: Baseado em estrutura de markdown com contexto preservado
- **Limpeza robusta**: RemoÃ§Ã£o de artefatos de conversÃ£o (tags HTML, imagens, etc)
- **Multi-provider**: Suporte nativo para Ollama, OpenAI e Google Gemini
- **Embeddings flexÃ­veis**: HuggingFace (local) e Ollama
- **Modularidade**: CÃ³digo organizado em mÃ³dulos reutilizÃ¡veis

---

## ğŸ—ï¸ Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PDF/TXT       â”‚
â”‚   Document      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Docling       â”‚
â”‚   (PDF â†’ MD)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Chunking      â”‚
â”‚   (Hybrid)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Cleaning      â”‚
â”‚   (Regex)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Embeddings    â”‚
â”‚ (HF or Ollama)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ChromaDB      â”‚
â”‚  (Vector Store) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Retrieval     â”‚
â”‚   (base_k=10)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Reranking     â”‚
â”‚ (CrossEncoder)  â”‚
â”‚   (top_n=4)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      LLM        â”‚
â”‚ (Ollama/API)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Response     â”‚
â”‚   + Sources     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Estrutura do Projeto

```
riza/
â”œâ”€â”€ app.py                          # Interface Streamlit principal
â”œâ”€â”€ Modelfile                       # ConfiguraÃ§Ã£o customizada para Ollama
â”œâ”€â”€ .env.example                    # Template de variÃ¡veis de ambiente
â”œâ”€â”€ requirements.txt                # DependÃªncias Python
â”œâ”€â”€ README.md                       # Esta documentaÃ§Ã£o
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                        # Documentos fonte
â”‚   â”‚   â”œâ”€â”€ galaxy_z_flip_7.pdf    # Manual de teste completo
â”‚   â”‚   â””â”€â”€ galaxy_z_flip_7_teste.pdf  # VersÃ£o reduzida para testes
â”‚   â”‚
â”‚   â””â”€â”€ processed/                  # Dados processados
â”‚       â”œâ”€â”€ manual_teste_markdown.md    # ConversÃ£o intermediÃ¡ria
â”‚       â””â”€â”€ chroma_db/              # Vector stores persistidos
â”‚           â””â”€â”€ [hash]/             # ColeÃ§Ã£o por documento
â”‚
â”œâ”€â”€ notebooks/                      # ExploraÃ§Ã£o e prototipagem
â”‚   â”œâ”€â”€ 01_data_ingestion.ipynb    # Teste de conversÃ£o PDF â†’ Markdown
â”‚   â”œâ”€â”€ 02_embedding_indexing.ipynb    # Experimentos com embeddings
â”‚   â””â”€â”€ 03_rag_pipeline.ipynb      # Desenvolvimento do pipeline RAG
â”‚
â””â”€â”€ src/                            # CÃ³digo de produÃ§Ã£o
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ data_processing.py          # Processamento e limpeza de documentos
    â”œâ”€â”€ vector_store.py             # Gerenciamento do ChromaDB
    â””â”€â”€ rag_components.py           # LLM, embeddings e retrieval
```

### OrganizaÃ§Ã£o

- **`notebooks/`**: Ambiente de experimentaÃ§Ã£o onde foram prototipados os primeiros modelos e testadas diferentes abordagens. Cada notebook representa uma etapa do desenvolvimento do pipeline.

- **`src/`**: CÃ³digo final modularizado e otimizado. ContÃ©m as implementaÃ§Ãµes definitivas das funÃ§Ãµes utilizadas no sistema.

- **`data/raw/`**: Armazena o manual do Galaxy Z Flip 7, usado inicialmente como caso de teste para validar o sistema de Q&A.

---

## ğŸ”§ Funcionalidades

### Implementadas

1. **IngestÃ£o de Documentos**
   - Suporte a PDF via Docling
   - ConversÃ£o inteligente para Markdown preservando estrutura
   - Processamento de headers e seÃ§Ãµes

2. **Chunking HÃ­brido**
   - DivisÃ£o baseada em estrutura (headers H2 e H3, por padrÃ£o, permitindo configuraÃ§Ã£o)
   - Chunk size configurÃ¡vel (padrÃ£o: 600 tokens)
   - Overlap inteligente (padrÃ£o: 100 tokens)
   - PreservaÃ§Ã£o de contexto hierÃ¡rquico

3. **Limpeza Robusta**
   - RemoÃ§Ã£o de tags HTML
   - EliminaÃ§Ã£o de comentÃ¡rios (`<!-- image -->`, etc)
   - NormalizaÃ§Ã£o de espaÃ§os e quebras de linha
   - Filtragem por tamanho mÃ­nimo

4. **Embeddings FlexÃ­veis**
   - **HuggingFace** (local): `all-MiniLM-L6-v2` (padrÃ£o)
   - **Ollama**: `nomic-embed-text`
   - FÃ¡cil extensÃ£o para outros modelos

5. **IndexaÃ§Ã£o Vetorial**
   - ChromaDB com persistÃªncia local
   - Collections isoladas por documento (evita interferÃªncia)
   - ReutilizaÃ§Ã£o automÃ¡tica de vector stores existentes

6. **Retrieval Inteligente**
   - Base retrieval: top-k documentos (configurÃ¡vel: 5-20)
   - **Compression Retriever**: Reranking com CrossEncoder
   - Modelo: `cross-encoder/ms-marco-MiniLM-L-6-v2`
   - Top-n apÃ³s reranking (configurÃ¡vel: 1-10)

7. **MÃºltiplos Provedores LLM**
   - **Ollama** (local): phi3:mini, llama3.2, mistral, etc
   - **OpenAI API**: GPT-3.5, GPT-4, GPT-4o
   - **Google Gemini API**: Gemini 2.5 Flash/Pro

8. **Interface Web**
   - Streamlit com design limpo
   - Upload de documentos
   - ConfiguraÃ§Ã£o de modelos em tempo real
   - HistÃ³rico de conversaÃ§Ã£o
   - VisualizaÃ§Ã£o de fontes utilizadas

9. **Rastreabilidade**
   - CitaÃ§Ã£o de trechos fonte
   - IndicaÃ§Ã£o de relevÃ¢ncia
   - TransparÃªncia na origem das respostas

### Trade-offs e DecisÃµes

| Aspecto | DecisÃ£o | Justificativa |
|---------|---------|---------------|
| **ConversÃ£o PDF** | Docling | Melhor preservaÃ§Ã£o de estrutura vs PyPDF |
| **Chunking** | HÃ­brido (headers + size) | Balanceia contexto semÃ¢ntico e tamanho |
| **Vector DB** | ChromaDB | Local, leve, suficiente para PoC |
| **Embedding padrÃ£o** | HuggingFace | ExecuÃ§Ã£o local, zero custo |
| **Reranking** | CrossEncoder | Melhoria significativa na relevÃ¢ncia |
| **LLM padrÃ£o** | Ollama local | Privacidade, custo zero, reprodutÃ­vel |

---

## ğŸš€ InstalaÃ§Ã£o

### PrÃ©-requisitos

- Python 3.10+
- 8GB+ RAM
- (Opcional) GPU para modelos locais maiores

### Passo 1: Clone o repositÃ³rio

```bash
git clone https://github.com/seu-usuario/riza.git
cd riza
```

### Passo 2: Crie ambiente virtual

```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows
```

### Passo 3: Instale dependÃªncias

```bash
pip install -r requirements.txt
```

### Passo 4: Configure Ollama (opcional)

Se quiser usar modelos locais:

```bash
# Instalar Ollama
curl -fsSL https://ollama.com/install.sh | sh  # Linux/Mac

# Baixar modelos
ollama pull phi3:mini
ollama pull nomic-embed-text
```

### Passo 5: Configure APIs (opcional)

Se quiser usar OpenAI ou Google:

```bash
cp .env.example .env
# Edite .env e adicione suas chaves:
# OPENAI_API_KEY=...
# GOOGLE_API_KEY=...
```

---

## ğŸ’» Como Usar

### Interface Web (Recomendado)

```bash
streamlit run app.py
```

Acesse `http://localhost:8501` e:

1. **Configure modelos** na sidebar:
   - Embedding: HuggingFace ou Ollama
   - LLM: Ollama, OpenAI ou Google
   - Ajuste temperature e parÃ¢metros de retrieval

2. **FaÃ§a upload de um PDF**

3. **Aguarde processamento** (primeira vez: ~2-5 min)

4. **Converse!** FaÃ§a perguntas sobre o documento

### ProgramÃ¡tico

```python
from src.data_processing import process_pdf_to_chunks
from src.vector_store import build_vector_store
from src.rag_components import (
    create_embedding_model,
    create_llm,
    create_compression_retriever,
    create_rag_chain,
    query_rag
)

# 1. Processar documento
chunks = process_pdf_to_chunks("data/raw/seu_documento.pdf")

# 2. Criar embeddings
embedding_model = create_embedding_model("huggingface", "all-MiniLM-L6-v2")

# 3. Indexar
vector_store = build_vector_store(
    chunks, 
    embedding_model, 
    "data/processed/chroma_db",
    collection_name="meu_doc"
)

# 4. Criar retriever com reranking
retriever = create_compression_retriever(vector_store, base_k=10, top_n=4)

# 5. Criar LLM
llm = create_llm("ollama", "phi3:mini", temperature=0.7)

# 6. Criar chain RAG
qa_chain = create_rag_chain(llm, retriever)

# 7. Fazer perguntas
response = query_rag(qa_chain, "Qual a capacidade da bateria?")
print(response["result"])
print(response["source_documents"])
```

---

## ğŸ› ï¸ Stack TecnolÃ³gica

### Bibliotecas Principais

| Componente | Biblioteca | VersÃ£o | FunÃ§Ã£o |
|------------|-----------|--------|--------|
| PDF Processing | Docling | 1.0+ | ConversÃ£o PDF â†’ Markdown |
| Text Splitting | LangChain | 0.3+ | Chunking inteligente |
| Embeddings | HuggingFace | 0.1+ | GeraÃ§Ã£o de vetores (local) |
| | Ollama | 0.2+ | Embeddings via API local |
| Vector DB | ChromaDB | 0.4+ | Armazenamento vetorial |
| Reranking | HuggingFace CrossEncoder | - | ReordenaÃ§Ã£o de resultados |
| LLM | LangChain Ollama | 0.2+ | Interface para modelos locais |
| | LangChain OpenAI | 0.2+ | Interface para GPT |
| | LangChain Google | 2.0+ | Interface para Gemini |
| Interface | Streamlit | 1.28+ | Web UI |
| Utilities | python-dotenv | 1.0+ | Gerenciamento de variÃ¡veis |

### Modelos

**Embeddings:**
- `all-MiniLM-L6-v2` (HuggingFace) - 384 dim, 22M params
- `nomic-embed-text` (Ollama) - 768 dim, otimizado para RAG

**Reranking:**
- `cross-encoder/ms-marco-MiniLM-L-6-v2` - Treinado em MS MARCO

**LLM (Ollama):**
- `phi3:mini` - 3.8B params, rÃ¡pido
- `llama3.2` - 3B params, balanceado
- `mistral` - 7B params, qualidade

**LLM (APIs):**
- OpenAI: GPT-3.5, GPT-4, GPT-4o
- Google: Gemini Pro, Gemini 1.5 Flash/Pro

---

## ğŸ”„ Pipeline RAG

### 1. PrÃ©-processamento

```python
# data_processing.py

def load_and_convert_to_markdown(file_path):
    # Converte PDF para Markdown usando Docling
    # Preserva estrutura de headers, parÃ¡grafos, listas
    pass

def chunk_text(markdown_text, chunk_size=600, chunk_overlap=100):
    # Chunking hÃ­brido:
    # 1. Divide por headers (H2, H3)
    # 2. Adiciona contexto hierÃ¡rquico como prefixo
    # 3. Subdivide chunks grandes recursivamente
    pass

def clean_and_filter_chunks(chunks, min_length=50):
    # Limpeza:
    # - Remove tags HTML
    # - Remove comentÃ¡rios (<!-- image -->)
    # - Normaliza espaÃ§os
    # - Filtra chunks muito pequenos
    pass
```

**ParÃ¢metros configurÃ¡veis:**
- `chunk_size`: 400-800 (padrÃ£o: 600)
- `chunk_overlap`: 50-150 (padrÃ£o: 100)
- `min_length`: 30-100 (padrÃ£o: 50)

### 2. IndexaÃ§Ã£o

```python
# vector_store.py

def build_vector_store(chunks, embedding_model, persist_directory, collection_name):
    # Cria collection Ãºnica por documento (hash MD5)
    # Gera embeddings para cada chunk
    # Persiste no ChromaDB
    # Permite reutilizaÃ§Ã£o em execuÃ§Ãµes futuras
    pass
```

**OtimizaÃ§Ãµes:**
- Collections isoladas (evita cross-contamination)
- PersistÃªncia automÃ¡tica
- DetecÃ§Ã£o de vector stores existentes

### 3. Retrieval + Reranking

```python
# rag_components.py

def create_compression_retriever(vector_store, base_k=10, top_n=4):
    # Pipeline de 2 estÃ¡gios:
    # Stage 1: Retrieval por similaridade de embeddings (base_k)
    # Stage 2: Reranking com CrossEncoder (top_n)
    pass
```


### 4. GeraÃ§Ã£o

```python
def create_rag_chain(llm, retriever, chain_type="stuff"):
    # Monta chain LangChain com:
    # - Retriever configurado
    # - LLM escolhido
    # - Return source documents
    pass

def query_rag(qa_chain, question):
    # Executa query usando .invoke()
    # Retorna resposta + documentos fonte
    pass
```

---

## ğŸ¯ DecisÃµes TÃ©cnicas

### 1. Por que Docling?

**Alternativas avaliadas:**
- PyPDF2: Perde formataÃ§Ã£o
- PyMuPDF: Bom mas complexo
- pdfplumber: Focado em tabelas
- **Docling**: Melhor preservaÃ§Ã£o de estrutura Markdown

**Vantagens:**
- MantÃ©m hierarquia de headers
- Detecta listas e tabelas
- ConversÃ£o limpa

### 2. Por que ChromaDB?

**Alternativas avaliadas:**
- Zero configuraÃ§Ã£o
- PersistÃªncia automÃ¡tica
- Isolamento por collection

### 3. Por que Compression Retriever?

**Problema:** Similarity search Ã© rÃ¡pido, mas nÃ£o tÃ£o preciso

**SoluÃ§Ã£o:** Two-stage retrieval
1. Top K (base_k=10)
2. Rerank with better model (top_n=4)

### 4. Por que mÃºltiplos provedores?

**Flexibilidade:**
- **Ollama**: Dev local, privacidade, zero custo
- **OpenAI**: MÃ¡xima qualidade
- **Google**: Bom custo-benefÃ­cio

### 5. SeguranÃ§a e Privacidade

**Dados sensÃ­veis:**
- Usar **apenas Ollama** (tudo local)
- Vector stores em `data/processed/`
- Nunca sai da mÃ¡quina

**APIs externas:**
- Dados trafegam para OpenAI/Google
- Avaliar compliance (LGPD, GDPR)
- Revisar termos de serviÃ§o

---

## âš™ï¸ ConfiguraÃ§Ã£o AvanÃ§ada

### CustomizaÃ§Ã£o via Modelfile (Ollama)

O arquivo `Modelfile` na raiz do projeto permite customizar o comportamento dos modelos Ollama:

```dockerfile
FROM {MODEL_NAME}

TEMPLATE """<|system|>
You are a {TYPE FUNC HERE} assistant specialized in {DOMAIN HERE}.
Your responses should be {STYLE HERE} and always {CONSTRAINT HERE}.<|end|>
<|user|>
{{ .Prompt }}<|end|>
<|assistant|>
"""

PARAMETER temperature {TEMPERATURE}
PARAMETER top_p {TOP_P}
PARAMETER stop "<|end|>"
PARAMETER stop "<|endoftext|>"

SYSTEM """You are {ROLE HERE}. {ADDITIONAL INSTRUCTIONS HERE}."""
```

**Exemplos de uso:**

**Assistente TÃ©cnico:**
```dockerfile
FROM phi3:mini

TEMPLATE """<|system|>
You are a technical support assistant specialized in smartphone troubleshooting.
Your responses should be clear, step-by-step and always reference the manual.<|end|>
<|user|>
{{ .Prompt }}<|end|>
<|assistant|>
"""

PARAMETER temperature 0.3
PARAMETER top_p 0.9

SYSTEM """You are a technical support specialist. Always provide safety warnings when relevant."""
```

**Assistente JurÃ­dico:**
```dockerfile
FROM llama3.2

TEMPLATE """<|system|>
You are a legal document analyst specialized in contract review.
Your responses should be formal, precise and always cite specific clauses.<|end|>
<|user|>
{{ .Prompt }}<|end|>
<|assistant|>
"""

PARAMETER temperature 0.1
PARAMETER top_p 0.85
```

**Para usar:**
```bash
ollama create assistente-tecnico -f Modelfile
# Use "assistente-tecnico" no RIZA
```

### Ajuste de ParÃ¢metros (RecomendaÃ§Ã£o)

**Chunking para documentos tÃ©cnicos:**
```python
chunks = process_pdf_to_chunks(
    file_path,
    chunk_size=800,      # Chunks maiores
    chunk_overlap=150,   # Mais overlap
    min_length=100       # Chunks mais substanciais
)
```

**Retrieval para documentos longos:**
```python
retriever = create_compression_retriever(
    vector_store,
    base_k=15,    # Busca mais ampla
    top_n=7       # Mais contexto
)
```

**LLM para respostas criativas:**
```python
llm = create_llm("ollama", "mistral", temperature=0.9)
```

---

## ğŸš§ PrÃ³ximos Passos

### Curto Prazo

1. **Fine-tuning com Unsloth/Axolotl**
   - Treinar phi3:mini em domÃ­nios especÃ­ficos
   - LoRA para customizaÃ§Ã£o de estilo/terminologia
   - Ãštil para: artigos cientÃ­ficos, documentos jurÃ­dicos
   - ImplementaÃ§Ã£o: Google Colab (GPU grÃ¡tis)

2. **Embeddings via API**
   - OpenAI: `text-embedding-3-small/large`
   - Cohere: `embed-multilingual-v3`
   - Voyage AI: `voyage-2`
   - ComparaÃ§Ã£o de qualidade/custo

3. **Mais Provedores LLM**
   - Anthropic Claude (via Bedrock ou direto)
   - Mistral API
   - Groq (inferÃªncia ultra-rÃ¡pida)
   - Together AI (modelos open-source)

### MÃ©dio Prazo

4. **RAG Multimodal Robusto**
   - ExtraÃ§Ã£o de imagens, diagramas, tabelas
   - DescriÃ§Ã£o automÃ¡tica de imagens (BLIP, LLaVA)
   - IndexaÃ§Ã£o conjunta texto+imagens
   - OCR para documentos escaneados

5. **Interface AvanÃ§ada**
   - Editor de chunks prÃ©-indexaÃ§Ã£o
   - ComparaÃ§Ã£o visual de embeddings (t-SNE/UMAP)
   - Ajuste de parÃ¢metros em tempo real
   - HistÃ³rico persistente de conversas
   - Export de resultados (PDF, Markdown)

6. **AvaliaÃ§Ã£o AutomÃ¡tica**
   - Dataset de Q&A ground-truth
   - MÃ©tricas: ROUGE, BLEU, BERTScore
   - Similarity entre resposta e fonte
   - A/B testing entre configuraÃ§Ãµes

### Longo Prazo

7. **IntegraÃ§Ã£o Web Search**
   - Fallback quando documento nÃ£o tem resposta
   - APIs: Brave Search, SerpAPI, Tavily
   - FusÃ£o RAG local + web results

8. **Agentic RAG**
   - ReAct pattern (reason + act)
   - Tool use (calculator, API calls)
   - Multi-step reasoning

9. **ProdutizaÃ§Ã£o**
   - API FastAPI
   - ContainerizaÃ§Ã£o (Docker)
   - CI/CD pipeline
   - Monitoring e logging (MLflow, W&B)
   - Rate limiting e autenticaÃ§Ã£o

---